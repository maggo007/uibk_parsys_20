#+options: ':nil *:t -:t ::t <:t H:3 \n:t ^:nil arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:t f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+options: timestamp:t title:t toc:t todo:t |:t
#+options: center:nil
#+title: Assignment 09
#+author: Markus Kopp
#+email: markus.kopp@student.uibk.ac.at
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 28.0.50 (Org mode 9.4)

#+latex_class: scrartcl
#+latex_class_options:
#+latex_header: \usepackage[margin=0.5in]{geometry}
#+latex_header_extra:
#+description:
#+keywords:
#+subtitle:
#+latex_compiler: pdflatex
#+date: \today
* Exercise 1
The goal of this exercise is to look through a given code and find computational hotspots that can be parallelized with openmp. So the first step is to run a performance analyzer on the sequential program.
I chose =perf record= that writes a profile with callstack that can be analyzed later by either =perf report= or in my case the program =hotspot= that used the same output generated by =perf=. Also important is to check the cache misses that can be introduced by the openmp pragmas. All the performance measurements were done on my laptop because the earlier exercises were running on similar times in the sequential versions. There is only one program source for both the sequental and parallel version with the only difference being that the parallel version is compiled with =-fopenmp=. The initial optimization flag of =O3= was kept.
** stats on sequential version
*** perf stat
With =perf stat -d ./real= we get the following output. Lets keep them in mind for the comparison later.
#+begin_example
Performance counter stats for './real':

         19.016,78 msec task-clock:u              #    0,999 CPUs utilized          
                 0      context-switches:u        #    0,000 K/sec                  
                 0      cpu-migrations:u          #    0,000 K/sec                  
           110.594      page-faults:u             #    0,006 M/sec                  
    46.086.278.492      cycles:u                  #    2,423 GHz                      (59,99%)
       807.048.326      stalled-cycles-frontend:u #    1,75% frontend cycles idle     (30,02%)
    30.325.994.862      stalled-cycles-backend:u  #   65,80% backend cycles idle      (30,03%)
    42.484.371.979      instructions:u            #    0,92  insn per cycle         
                                                  #    0,71  stalled cycles per insn  (40,02%)
     1.646.773.309      branches:u                #   86,596 M/sec                    (50,02%)
         9.150.206      branch-misses:u           #    0,56% of all branches          (60,01%)
    21.247.551.485      L1-dcache-loads:u         # 1117,305 M/sec                    (60,00%)
     1.296.131.552      L1-dcache-load-misses:u   #    6,10% of all L1-dcache accesses  (59,98%)
     2.373.412.628      LLC-loads:u               #  124,806 M/sec                    (59,97%)
        41.404.141      LLC-load-misses:u         #    1,74% of all LL-cache accesses  (59,98%)

      19,027850733 seconds time elapsed

      18,462246000 seconds user
       0,305602000 seconds sys

#+end_example
*** perf report
After collecting the data with =perf record -g ./real= the output is saved in =perf.data= file. It can be viewed by =perf report= or by a different app =hotspot= that needs to be installed separately. Lets have a look at what the show us.
[[./graph/perfreportseq01.png]]
[[./graph/perfreportseq02.png]]
The codelines are already referring to the changed code. The assembler code is already showing a none vectorized version of the call. So we can add the simd pragma. The original version already had mulpd calls but spent a lot of time on them. The hotspot program shows the same codelines.
[[./graph/hotspotseq01.png]]
** changes to sequential
First thing that was done was to add all the simd pragmas where possible. Because the original loops in most of the function calls were dependent on the previous completion we were not able to add a parallel for pragma on the outermost loop without corrupting the output and failing verification the first approach was to add the parallel for pragma on the innermost loop of a three level nested loop. This did not decrease the execution time because there was to much overhead. For example when the two outer loops have =i1= and =i2= that loop over 256 each than you have to create threads and wait for them all to finish for $256*256=65536$ times. This lead to openmp calls taken the majority of the calls in the program.
So there needs to be some changes done. The dependency on a values is only calculated where needed and replaces an array with all the needed values.
#+begin_src C :eval never-export
  static void psinv(void *or, void *ou, int n1, int n2, int n3,
                    double c[4], int k)
  {
    double (*r)[n2][n1] = (double (*)[n2][n1])or;
    double (*u)[n2][n1] = (double (*)[n2][n1])ou;

    int i3, i2, i1;

    double r1prev, r2prev, r1curr, r2curr, r1next, r2next;

    if (timeron) timer_start(T_psinv);
  #pragma omp parallel for collapse(2) schedule(static,64)
    for (i3 = 1; i3 < n3-1; i3++) {
      for (i2 = 1; i2 < n2-1; i2++) {

          r1prev = r[i3][i2-1][0] + r[i3][i2+1][0] + r[i3-1][i2][0] + r[i3+1][i2][0];
          r2prev = r[i3-1][i2-1][0] + r[i3-1][i2+1][0] + r[i3+1][i2-1][0] + r[i3+1][i2+1][0];
          r1curr = r[i3][i2-1][1] + r[i3][i2+1][1] + r[i3-1][i2][1] + r[i3+1][i2][1];
          r2curr = r[i3-1][i2-1][1] + r[i3-1][i2+1][1] + r[i3+1][i2-1][1] + r[i3+1][i2+1][1];

        for (i1 = 1; i1 < n1-1; i1++) {
          r1next = r[i3][i2-1][i1+1] + r[i3][i2+1][i1+1] + r[i3-1][i2][i1+1] + r[i3+1][i2][i1+1];
          r2next = r[i3-1][i2-1][i1+1] + r[i3-1][i2+1][i1+1] + r[i3+1][i2-1][i1+1] + r[i3+1][i2+1][i1+1];

          u[i3][i2][i1] = u[i3][i2][i1]
                        + c[0] * r[i3][i2][i1]
                        + c[1] * ( r[i3][i2][i1-1] + r[i3][i2][i1+1]
                                 + r1curr )
                        + c[2] * ( r2curr + r1prev + r1next );
          //--------------------------------------------------------------------
          // Assume c[3] = 0    (Enable line below if c[3] not= 0)
          //--------------------------------------------------------------------
          //            + c[3] * ( r2[i1-1] + r2[i1+1] )
          //--------------------------------------------------------------------

          r1prev = r1curr;
          r2prev = r2curr;
          r1curr = r1next;
          r2curr = r2next;
        }
      }
    }
    ...
  #+end_src

  Comparing to the original version.
  #+begin_src C :eval never-export
    static void psinv(void *or, void *ou, int n1, int n2, int n3,
                      double c[4], int k)
    {
      double (*r)[n2][n1] = (double (*)[n2][n1])or;
      double (*u)[n2][n1] = (double (*)[n2][n1])ou;

      int i3, i2, i1;

      double r1[M], r2[M];

      if (timeron) timer_start(T_psinv);
      for (i3 = 1; i3 < n3-1; i3++) {
        for (i2 = 1; i2 < n2-1; i2++) {
          for (i1 = 0; i1 < n1; i1++) {
            r1[i1] = r[i3][i2-1][i1] + r[i3][i2+1][i1]
                   + r[i3-1][i2][i1] + r[i3+1][i2][i1];
            r2[i1] = r[i3-1][i2-1][i1] + r[i3-1][i2+1][i1]
                   + r[i3+1][i2-1][i1] + r[i3+1][i2+1][i1];
          }
          for (i1 = 1; i1 < n1-1; i1++) {
            u[i3][i2][i1] = u[i3][i2][i1]
                          + c[0] * r[i3][i2][i1]
                          + c[1] * ( r[i3][i2][i1-1] + r[i3][i2][i1+1]
                                   + r1[i1] )
                          + c[2] * ( r2[i1] + r1[i1-1] + r1[i1+1] );
            //--------------------------------------------------------------------
            // Assume c[3] = 0    (Enable line below if c[3] not= 0)
            //--------------------------------------------------------------------
            //            + c[3] * ( r2[i1-1] + r2[i1+1] )
            //--------------------------------------------------------------------
          }
        }
      }
      ...
  #+end_src
A similar pattern was found in most of the calls that were showing up in the hotspot report.
** stats on parallel version
*** perf stat
#+begin_example
Performance counter stats for './real_omp':

         32.101,26 msec task-clock:u              #    2,662 CPUs utilized          
                 0      context-switches:u        #    0,000 K/sec                  
                 0      cpu-migrations:u          #    0,000 K/sec                  
           110.626      page-faults:u             #    0,003 M/sec                  
    56.878.389.267      cycles:u                  #    1,772 GHz                      (59,95%)
       567.540.512      stalled-cycles-frontend:u #    1,00% frontend cycles idle     (29,97%)
    40.449.294.522      stalled-cycles-backend:u  #   71,12% backend cycles idle      (29,98%)
    43.521.061.030      instructions:u            #    0,77  insn per cycle         
                                                  #    0,93  stalled cycles per insn  (40,04%)
     2.073.013.758      branches:u                #   64,577 M/sec                    (50,05%)
         9.779.759      branch-misses:u           #    0,47% of all branches          (60,08%)
    21.964.393.783      L1-dcache-loads:u         #  684,222 M/sec                    (60,08%)
     1.335.611.537      L1-dcache-load-misses:u   #    6,08% of all L1-dcache accesses  (60,06%)
     2.373.786.698      LLC-loads:u               #   73,947 M/sec                    (60,05%)
        68.344.603      LLC-load-misses:u         #    2,88% of all LL-cache accesses  (59,96%)

      12,060290055 seconds time elapsed

      30,870296000 seconds user
       0,718952000 seconds sys
#+end_example

It shows a speedup but not all four of my cores are used. So it is not possible to have perfect speedup in our case. Cache misses are about the same but the original sequential version was better. This was the trade-off for introducing parallel execution here.
*** execution time on LCC2
Because we only measure 9 values in total there is no graph this time.
#+ATTR_LATEX: :width \textwidth :caption Execution time on lcc2 in seconds from benchmk time
|    seq |   omp1 |  omp2 |   omp3 |  omp4 |   omp5 |   omp6 |   omp7 |  omp8 |
|--------+--------+-------+--------+-------+--------+--------+--------+-------|
| 12.575 | 12.702 | 8.897 | 12.452 | 8.538 | 11.381 | 10.040 | 11.260 | 8.682 |
*** perf stat on LCC2
We can see that cachmisses increase with the number of threads. That's why it is not scaling very well and CPU usage is also never maxed because only a selected calls very parallelized.

**** seq
#+begin_example
 Performance counter stats for './real':

         15,590.34 msec task-clock:u              #    0.995 CPUs utilized          
                 0      context-switches:u        #    0.000 K/sec                  
                 0      cpu-migrations:u          #    0.000 K/sec                  
           110,720      page-faults:u             #    0.007 M/sec                  
    37,023,266,951      cycles:u                  #    2.375 GHz                      (25.00%)
    42,352,979,477      instructions:u            #    1.14  insn per cycle           (37.51%)
     1,653,416,869      branches:u                #  106.054 M/sec                    (37.50%)
         7,111,621      branch-misses:u           #    0.43% of all branches          (37.51%)
    20,089,024,549      L1-dcache-loads:u         # 1288.556 M/sec                    (25.00%)
       892,049,893      L1-dcache-load-misses:u   #    4.44% of all L1-dcache hits    (25.00%)
       870,839,650      LLC-loads:u               #   55.858 M/sec                    (25.00%)
        26,854,928      LLC-load-misses:u         #    3.08% of all LL-cache hits     (24.99%)

      15.661330582 seconds time elapsed

      15.143668000 seconds user
       0.447642000 seconds sys
#+end_example
**** omp1
#+begin_example
 Performance counter stats for './real_omp':

         15,724.13 msec task-clock:u              #    0.999 CPUs utilized          
                 0      context-switches:u        #    0.000 K/sec                  
                 0      cpu-migrations:u          #    0.000 K/sec                  
           110,779      page-faults:u             #    0.007 M/sec                  
    37,334,291,351      cycles:u                  #    2.374 GHz                      (25.00%)
    42,410,863,888      instructions:u            #    1.14  insn per cycle           (37.49%)
     1,644,373,785      branches:u                #  104.576 M/sec                    (37.52%)
         6,658,770      branch-misses:u           #    0.40% of all branches          (37.50%)
    20,095,543,125      L1-dcache-loads:u         # 1278.006 M/sec                    (25.00%)
       915,192,337      L1-dcache-load-misses:u   #    4.55% of all L1-dcache hits    (25.00%)
       875,596,280      LLC-loads:u               #   55.685 M/sec                    (24.99%)
        23,146,876      LLC-load-misses:u         #    2.64% of all LL-cache hits     (24.99%)

      15.740153378 seconds time elapsed

      15.293026000 seconds user
       0.432029000 seconds sys
#+end_example
**** omp2
#+begin_example
 Performance counter stats for './real_omp':

         17,594.32 msec task-clock:u              #    1.533 CPUs utilized          
                 0      context-switches:u        #    0.000 K/sec                  
                 0      cpu-migrations:u          #    0.000 K/sec                  
           111,064      page-faults:u             #    0.006 M/sec                  
    41,711,917,885      cycles:u                  #    2.371 GHz                      (24.98%)
    42,948,866,377      instructions:u            #    1.03  insn per cycle           (37.49%)
     1,791,668,114      branches:u                #  101.832 M/sec                    (37.50%)
         6,675,628      branch-misses:u           #    0.37% of all branches          (37.51%)
    20,153,883,645      L1-dcache-loads:u         # 1145.477 M/sec                    (25.01%)
     1,072,046,555      L1-dcache-load-misses:u   #    5.32% of all L1-dcache hits    (25.01%)
       902,621,513      LLC-loads:u               #   51.302 M/sec                    (25.00%)
        28,657,178      LLC-load-misses:u         #    3.17% of all LL-cache hits     (24.99%)

      11.474997451 seconds time elapsed

      17.059198000 seconds user
       0.537226000 seconds sys
#+end_example
**** omp3
#+begin_example
 Performance counter stats for './real_omp':

         30,982.87 msec task-clock:u              #    2.041 CPUs utilized          
                 0      context-switches:u        #    0.000 K/sec                  
                 0      cpu-migrations:u          #    0.000 K/sec                  
           110,994      page-faults:u             #    0.004 M/sec                  
    74,390,467,389      cycles:u                  #    2.401 GHz                      (25.00%)
    43,625,739,554      instructions:u            #    0.59  insn per cycle           (37.50%)
     1,968,463,070      branches:u                #   63.534 M/sec                    (37.50%)
         6,715,560      branch-misses:u           #    0.34% of all branches          (37.48%)
    20,226,187,840      L1-dcache-loads:u         #  652.818 M/sec                    (24.99%)
     1,366,513,046      L1-dcache-load-misses:u   #    6.76% of all L1-dcache hits    (25.00%)
       894,477,567      LLC-loads:u               #   28.870 M/sec                    (25.01%)
        64,702,222      LLC-load-misses:u         #    7.23% of all LL-cache hits     (25.01%)

      15.180661559 seconds time elapsed

      30.392587000 seconds user
       0.594796000 seconds sys
#+end_example
**** omp4
#+begin_example
 Performance counter stats for './real_omp':

         27,352.18 msec task-clock:u              #    2.491 CPUs utilized          
                 0      context-switches:u        #    0.000 K/sec                  
                 0      cpu-migrations:u          #    0.000 K/sec                  
           111,054      page-faults:u             #    0.004 M/sec                  
    65,288,232,006      cycles:u                  #    2.387 GHz                      (25.02%)
    43,943,223,689      instructions:u            #    0.67  insn per cycle           (37.51%)
     2,069,733,833      branches:u                #   75.670 M/sec                    (37.49%)
         6,690,032      branch-misses:u           #    0.32% of all branches          (37.48%)
    20,297,582,292      L1-dcache-loads:u         #  742.083 M/sec                    (25.01%)
       997,775,066      L1-dcache-load-misses:u   #    4.92% of all L1-dcache hits    (25.00%)
       899,997,776      LLC-loads:u               #   32.904 M/sec                    (24.99%)
        39,019,071      LLC-load-misses:u         #    4.34% of all LL-cache hits     (25.00%)

      10.980268755 seconds time elapsed

      26.677452000 seconds user
       0.680190000 seconds sys
#+end_example
**** omp5
#+begin_example
 Performance counter stats for './real_omp':

         43,889.68 msec task-clock:u              #    3.133 CPUs utilized          
                 0      context-switches:u        #    0.000 K/sec                  
                 0      cpu-migrations:u          #    0.000 K/sec                  
           110,794      page-faults:u             #    0.003 M/sec                  
   105,150,435,094      cycles:u                  #    2.396 GHz                      (24.99%)
    44,745,608,437      instructions:u            #    0.43  insn per cycle           (37.49%)
     2,330,615,927      branches:u                #   53.102 M/sec                    (37.51%)
         6,673,645      branch-misses:u           #    0.29% of all branches          (37.52%)
    20,492,587,142      L1-dcache-loads:u         #  466.911 M/sec                    (24.98%)
     1,320,294,062      L1-dcache-load-misses:u   #    6.44% of all L1-dcache hits    (25.00%)
       896,076,377      LLC-loads:u               #   20.417 M/sec                    (25.01%)
        91,362,907      LLC-load-misses:u         #   10.20% of all LL-cache hits     (24.99%)

      14.009661220 seconds time elapsed

      43.152480000 seconds user
       0.746423000 seconds sys
#+end_example
**** omp6
#+begin_example
 Performance counter stats for './real_omp':

         45,691.91 msec task-clock:u              #    3.638 CPUs utilized          
                 0      context-switches:u        #    0.000 K/sec                  
                 0      cpu-migrations:u          #    0.000 K/sec                  
           110,884      page-faults:u             #    0.002 M/sec                  
   108,857,808,469      cycles:u                  #    2.382 GHz                      (24.99%)
    44,998,346,982      instructions:u            #    0.41  insn per cycle           (37.49%)
     2,388,578,362      branches:u                #   52.276 M/sec                    (37.49%)
         6,720,427      branch-misses:u           #    0.28% of all branches          (37.50%)
    20,480,210,548      L1-dcache-loads:u         #  448.224 M/sec                    (25.00%)
     1,288,002,674      L1-dcache-load-misses:u   #    6.29% of all L1-dcache hits    (25.01%)
       898,460,082      LLC-loads:u               #   19.663 M/sec                    (25.01%)
        85,150,291      LLC-load-misses:u         #    9.48% of all LL-cache hits     (25.00%)

      12.558358562 seconds time elapsed

      44.885194000 seconds user
       0.819350000 seconds sys
#+end_example
**** omp7
#+begin_example
 Performance counter stats for './real_omp':

         57,681.50 msec task-clock:u              #    4.162 CPUs utilized          
                 0      context-switches:u        #    0.000 K/sec                  
                 0      cpu-migrations:u          #    0.000 K/sec                  
           110,808      page-faults:u             #    0.002 M/sec                  
   137,197,126,246      cycles:u                  #    2.379 GHz                      (25.00%)
    45,778,103,594      instructions:u            #    0.33  insn per cycle           (37.49%)
     2,570,736,859      branches:u                #   44.568 M/sec                    (37.50%)
         6,692,259      branch-misses:u           #    0.26% of all branches          (37.50%)
    20,529,693,712      L1-dcache-loads:u         #  355.915 M/sec                    (25.02%)
     1,324,045,607      L1-dcache-load-misses:u   #    6.45% of all L1-dcache hits    (25.01%)
       897,184,385      LLC-loads:u               #   15.554 M/sec                    (24.99%)
       120,840,743      LLC-load-misses:u         #   13.47% of all LL-cache hits     (24.99%)

      13.859019007 seconds time elapsed

      56.735429000 seconds user
       0.960702000 seconds sys
#+end_example
**** omp8
#+begin_example
 Performance counter stats for './real_omp':

         50,116.19 msec task-clock:u              #    4.508 CPUs utilized          
                 0      context-switches:u        #    0.000 K/sec                  
                 0      cpu-migrations:u          #    0.000 K/sec                  
           110,974      page-faults:u             #    0.002 M/sec                  
   117,881,879,069      cycles:u                  #    2.352 GHz                      (25.00%)
    46,043,392,988      instructions:u            #    0.39  insn per cycle           (37.50%)
     2,701,120,893      branches:u                #   53.897 M/sec                    (37.47%)
         6,720,409      branch-misses:u           #    0.25% of all branches          (37.49%)
    20,620,857,266      L1-dcache-loads:u         #  411.461 M/sec                    (24.99%)
     1,143,809,588      L1-dcache-load-misses:u   #    5.55% of all L1-dcache hits    (24.98%)
       895,921,641      LLC-loads:u               #   17.877 M/sec                    (25.01%)
        79,826,785      LLC-load-misses:u         #    8.91% of all LL-cache hits     (25.05%)

      11.116540681 seconds time elapsed

      49.025619000 seconds user
       1.099722000 seconds sys
#+end_example
