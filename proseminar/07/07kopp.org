#+options: ':nil *:t -:t ::t <:t H:3 \n:t ^:nil arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:t f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+options: timestamp:t title:t toc:t todo:t |:t
#+options: center:nil
#+title: Assignment 07
#+author: Markus Kopp
#+email: markus.kopp@student.uibk.ac.at
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 28.0.50 (Org mode 9.4)

#+latex_class: scrartcl
#+latex_class_options:
#+latex_header: \usepackage[margin=0.5in]{geometry}
#+latex_header_extra:
#+description:
#+keywords:
#+subtitle:
#+latex_compiler: pdflatex
#+date: \today
* Exercise 1
By building a hybrid solution that utilizes mpi and openmp you just have to put a pragma at the correct position and make sure that the ranks that run on the node are not bound to a core because threads will not be able to run on our eight cores.

** heat_stencil_2d_mpi_hybrid
The pragma is put before the calculation. To each patch that a rank is working on is split up to the available threads.
#+begin_src C :eval never-export
  //some parts are copied from our old solutions in parallel openCL course https://git.uibk.ac.at/csat2062/parallel_local

  #include <mpi.h>
  #include <omp.h>
  #include <stdio.h>
  #include <stdlib.h>

  typedef double value_t;

  #define RESOLUTION 120
  #define DIMENSIONS 2

  enum mode {SEND, RECEIVE};

  // -- vector utilities --

  typedef value_t *Vector;

  Vector createVector(int N, int M);

  void releaseVector(Vector m);

  void printTemperature(Vector m, int N, int M);

  void neighbourMPI(int dimension, int* arraysize, int* subsize, int* substart, int* neighbour, MPI_Comm cartesian, double* sendbuffer, enum mode mode);

  // -- simulation code ---

  int main(int argc, char **argv) {
    int rank;
    int size;
    int partsize_row;
    int partsize_col;
    int dim[2] = {0,0};
    int periodic[2], coord[2];
    MPI_Comm cart;
    //MPI_Datatype subarray;
    MPI_Datatype gather, gather2;
    if (MPI_Init(&argc, &argv)){
      printf("init failed");
      exit(EXIT_FAILURE);
    }
    if (MPI_Comm_rank(MPI_COMM_WORLD, &rank)){
      printf("cant get mpi rank");
      exit(EXIT_FAILURE);
    }
    if (MPI_Comm_size(MPI_COMM_WORLD, &size)){
      printf("can't get mpi size");
      exit(EXIT_FAILURE);
    }
    int ompnum;
  #pragma omp parallel
    {
      ompnum = omp_get_num_threads();
    }


    printf("rank=%d with size=%d and threads=%d\n", rank, size, ompnum);
    // 'parsing' optional input parameter = problem size
    int N = 1000;
    int M = 1000;
    int T = 100;
    if (argc == 4) {
      N = atoi(argv[1]);
      M = atoi(argv[2]);
      T = atoi(argv[3]);
    }

    /* if (N % size != 0) { */
    /*   printf("size not ok for problem size. not dividable without remainder\n"); */
    /*   exit(EXIT_FAILURE); */
    /* } */

    int arraysize[2] = {M,N};

    printf("Computing heat-distribution for room size N=%d x %d for T=%d timesteps\n", N,M, T);

    // ---------- setup ----------

    // create a buffer for storing temperature fields
    Vector A = createVector(N,M);

    // set up initial conditions in A
    for (int row = 0; row < M; ++row) {
      for (int col = 0; col < N; ++col) {
        A[col + row*N] = 273; // temperature is 0Â° C everywhere (273 K)
      }
    }
    if (MPI_Dims_create(size, 2, dim)){
      printf("can't create cartesian ranks");
      exit(EXIT_FAILURE);
    }

    printf("dims=%d %d\n", dim[0], dim[1]);
    if (M % dim[0] != 0 || N % dim[1] != 0){
      printf("size not ok for problem size. not dividable without remainder\n");
      exit(EXIT_FAILURE);
    }
    partsize_row = M / dim[0];
    partsize_col = N / dim[1];
    periodic[0] = 0;
    periodic[1] = 0;
    int reorder = 1;
    if (MPI_Cart_create(MPI_COMM_WORLD, 2, dim, periodic, reorder, &cart)){
      printf("can't create cartesian communicator");
      exit(EXIT_FAILURE);
    }

    if (MPI_Cart_coords(cart, rank, DIMENSIONS, coord)){
      printf("can't query cartesian ranks");
      exit(EXIT_FAILURE);
    }

    // and there is a heat source in one corner
    int source_row = M / 4;
    int source_col = N / 4;
    A[source_row * N + source_col] = 273 + 60;

    printf("Initial:\n");
    printTemperature(A, N, M);
    printf("\n");

    // ---------- compute ----------

    // create a second buffer for the computation
    Vector B = createVector(N,M);

    long long start_row = coord[0] * partsize_row;
    long long stop_row = coord[0] * partsize_row + partsize_row;
    long long start_col = coord[1] * partsize_col;
    long long stop_col = coord[1] * partsize_col + partsize_col;

    printf("rank %d is at coord(%d,%d) with array start_row=%lld stop_row=%lld start_col=%lld stop_col=%lld\n", rank, coord[0], coord[1], start_row, stop_row, start_col, stop_col);
    int neighbour[DIMENSIONS];
    int subsize[DIMENSIONS];
    int substart[DIMENSIONS];
    // for each time step ..
    for (int t = 0; t < T; t++) {
      // .. we propagate the temperature

      if((coord[0]+coord[1]) % 2 == 0){
        // send to left neighbour
        if (coord[1] != 0){
          subsize[0] = partsize_row;
          subsize[1] = 1;
          substart[0] = start_row;
          substart[1] = start_col;
          neighbour[0] = coord[0];
          neighbour[1] = coord[1]-1;
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, SEND);
        }

        // receive from right neighbour
        if (coord[1] < dim[1]-1){
          subsize[0] = partsize_row;
          subsize[1] = 1;
          substart[0] = start_row;
          substart[1] = stop_col;
          neighbour[0] = coord[0];
          neighbour[1] = coord[1]+1;
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, RECEIVE);
        }

        // send to right  neighbour
        if (coord[1] < dim[1]-1){
          subsize[0] = partsize_row;
          subsize[1] = 1;
          substart[0] = start_row;
          substart[1] = stop_col-1;
          neighbour[0] = coord[0];
          neighbour[1] = coord[1]+1;
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, SEND);
        }

        // receive from left neighbour
        if (coord[1] != 0){
          subsize[0] = partsize_row;
          subsize[1] = 1;
          substart[0] = start_row;
          substart[1] = start_col-1;
          neighbour[0] = coord[0];
          neighbour[1] = coord[1]-1;
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, RECEIVE);
        }

        // send to bottom neighbour
        if (coord[0] < dim[0]-1){
          subsize[0] = 1;
          subsize[1] = partsize_col;
          substart[0] = stop_row-1;
          substart[1] = start_col;
          neighbour[0] = coord[0]+1;
          neighbour[1] = coord[1];
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, SEND);
        }
        // receive from top neighbour
        if (coord[0] != 0){
          subsize[0] = 1;
          subsize[1] = partsize_col;
          substart[0] = start_row-1;
          substart[1] = start_col;
          neighbour[0] = coord[0]-1;
          neighbour[1] = coord[1];
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, RECEIVE);
        }
        // send to top neighbour
        if (coord[0] != 0){
          subsize[0] = 1;
          subsize[1] = partsize_col;
          substart[0] = start_row;
          substart[1] = start_col;
          neighbour[0] = coord[0]-1;
          neighbour[1] = coord[1];
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, SEND);
        }
        // receive from bottom neighbour
        if (coord[0] < dim[0]-1){
          subsize[0] = 1;
          subsize[1] = partsize_col;
          substart[0] = stop_row;
          substart[1] = start_col;
          neighbour[0] = coord[0]+1;
          neighbour[1] = coord[1];
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, RECEIVE);
        }


      }else{

        // receive from right neighbour
        if (coord[1] < dim[1]-1){
          subsize[0] = partsize_row;
          subsize[1] = 1;
          substart[0] = start_row;
          substart[1] = stop_col;
          neighbour[0] = coord[0];
          neighbour[1] = coord[1]+1;
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, RECEIVE);
        }

        // send to left neighbour
        if (coord[1] != 0){
          subsize[0] = partsize_row;
          subsize[1] = 1;
          substart[0] = start_row;
          substart[1] = start_col;
          neighbour[0] = coord[0];
          neighbour[1] = coord[1]-1;
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, SEND);
        }


        // receive from left neighbour
        if (coord[1] != 0){
          subsize[0] = partsize_row;
          subsize[1] = 1;
          substart[0] = start_row;
          substart[1] = start_col-1;
          neighbour[0] = coord[0];
          neighbour[1] = coord[1]-1;
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, RECEIVE);
        }

        // send to right  neighbour
        if (coord[1] < dim[1]-1){
          subsize[0] = partsize_row;
          subsize[1] = 1;
          substart[0] = start_row;
          substart[1] = stop_col-1;
          neighbour[0] = coord[0];
          neighbour[1] = coord[1]+1;
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, SEND);
        }



        // receive from top neighbour
        if (coord[0] != 0){
          subsize[0] = 1;
          subsize[1] = partsize_col;
          substart[0] = start_row-1;
          substart[1] = start_col;
          neighbour[0] = coord[0]-1;
          neighbour[1] = coord[1];
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, RECEIVE);
        }
        // send to bottom neighbour
        if (coord[0] < dim[0]-1){
          subsize[0] = 1;
          subsize[1] = partsize_col;
          substart[0] = stop_row-1;
          substart[1] = start_col;
          neighbour[0] = coord[0]+1;
          neighbour[1] = coord[1];
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, SEND);
        }



        // receive from bottom neighbour
        if (coord[0] < dim[0]-1){
          subsize[0] = 1;
          subsize[1] = partsize_col;
          substart[0] = stop_row;
          substart[1] = start_col;
          neighbour[0] = coord[0]+1;
          neighbour[1] = coord[1];
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, RECEIVE);
        }
        // send to top neighbour
        if (coord[0] != 0){
          subsize[0] = 1;
          subsize[1] = partsize_col;
          substart[0] = start_row;
          substart[1] = start_col;
          neighbour[0] = coord[0]-1;
          neighbour[1] = coord[1];
          neighbourMPI(DIMENSIONS, arraysize, subsize, substart, neighbour, cart, A, SEND);
        }
      }
  #pragma omp parallel for collapse(2)
      for (int row = start_row; row < stop_row; row++) {
        for (int col = start_col; col < stop_col; col++) {
          // center stays constant (the heat is still on)
          if (col == source_col && row == source_row) {
            B[row*N+col] = A[col+row*N];
            continue;
          }

          // get current temperature at (x,y)
          value_t tc = A[row*N+col];

          // get temperatures left/right and up/down
          value_t tl = ( col !=  0  ) ? A[row*N+(col-1)] : tc;
          value_t tr = ( col != N-1 ) ? A[row*N+(col+1)] : tc;
          value_t tu = ( row !=  0  ) ? A[(row-1)*N+col] : tc;
          value_t td = ( row != M-1 ) ? A[(row+1)*N+col] : tc;

          // update temperature at current point
          B[row*N+col] = tc + 1.0/5 * (tl + tr + tu + td + (-4*tc));

        }
      }

      // swap matrices (just pointers, not content)
      Vector H = A;
      A = B;
      B = H;

      // show intermediate step
      if (!(t % 1000)) {
        printf("Step t=%d:\n", t);
        printTemperature(A, N, M);
        printf("\n");
      }
    }


    //int subsize[2] = {partsize_row, partsize_col};
    subsize[0] = partsize_row;
    subsize[1] = partsize_col;
    //int substart[2] = {start_row, start_col};
    substart[0] = start_row;
    substart[1] = start_col;
    int sendcount[size];
    int displacement[size];
    for (int i=0; i < size; ++i) {
      sendcount[i]=1;
    }
    int colcount = 0;
    int rowcount = 0;
    for (int i=0; i < size; ++i) {

      displacement[i]=colcount + rowcount;
      colcount += partsize_col;
      if ((i+1) % dim[1] == 0){
        colcount = 0;
        rowcount += partsize_row * N;
      }
    }

    //printf("subsize %d %d start %d %d\n",subsize[0], subsize[1], substart[0], substart[1] );
    if (MPI_Type_create_subarray(DIMENSIONS, arraysize, subsize, substart, MPI_ORDER_C, MPI_DOUBLE, &gather)){
      printf("can't create subarray");
      exit(EXIT_FAILURE);
    }
    if (MPI_Type_commit(&gather)){
      printf("can't commit mpi type");
      exit(EXIT_FAILURE);
    }
    if (MPI_Type_create_resized(gather, 0, 1*sizeof(double), &gather2)){
      printf("can't create resized array");
      exit(EXIT_FAILURE);
    }
    if (MPI_Type_commit(&gather2)){
      printf("can't commit mpi type");
      exit(EXIT_FAILURE);
    }

    if (MPI_Gatherv(A, 1, gather, A, sendcount, displacement, gather2, 0, MPI_COMM_WORLD)){
      printf("failure at gather");
      exit(EXIT_FAILURE);
    }

    if (MPI_Type_free(&gather)){
      printf("failure mpi type free");
      exit(EXIT_FAILURE);
    }
    if (MPI_Type_free(&gather2)){
      printf("failure mpi type free");
      exit(EXIT_FAILURE);
    }
    // ---------- check ----------



    int success = 1;
    if (rank == 0){
      printf("Final:\n");
      printTemperature(A, N, M);
      printf("\n");
      for(int y = 0; y<M; y++) {
        for(int x = 0; x<N; x++) {
          value_t temp = A[y*N+x];
          if (273 <= temp && temp <= 273+60) continue;
          success = 0;
          break;
        }
      }
      printf("Verification: %s\n", (success) ? "OK" : "FAILED");
    }



    // ---------- cleanup ----------

    releaseVector(A);
    releaseVector(B);


    if (MPI_Finalize()){
      printf("failure at mpi finalize");
      exit(EXIT_FAILURE);
    }


    // done
    return (success) ? EXIT_SUCCESS : EXIT_FAILURE;
  }

  Vector createVector(int N, int M) {
    // create data and index vector
    Vector mallocaddress = malloc(sizeof(value_t) * N * M);
    if (mallocaddress != 0){
      return mallocaddress;
    }else {
      printf("malloc failure");
      exit(EXIT_FAILURE);
    }
  }

  void releaseVector(Vector m) { free(m); }


  //taken from old parallel course https://git.uibk.ac.at/csat2062/parallel_local
  void printTemperature(Vector m, int N, int M) {
    const char* colors = " .-:=+*#%@";
    const int numColors = 10;

    // boundaries for temperature (for simplicity hard-coded)
    const value_t max = 273 + 30;
    const value_t min = 273 + 0;

    // set the 'render' resolution
    int H = 30;
    int W = 60;

    // step size in each dimension
    int sH = M/H;
    int sW = N/W;


    // upper wall
    for(int i=0; i<W+2; i++) {
      printf("X");
    }
    printf("\n");

    // room
    for(int i=0; i<H; i++) {
      // left wall
      printf("X");
      // actual room
      for(int j=0; j<W; j++) {

        // get max temperature in this tile
        value_t max_t = 0;
        for(int x=sH*i; x<sH*i+sH; x++) {
          for(int y=sW*j; y<sW*j+sW; y++) {
            max_t = (max_t < m[x*N+y]) ? m[x*N+y] : max_t;
          }
        }
        value_t temp = max_t;

        // pick the 'color'
        int c = ((temp - min) / (max - min)) * numColors;
        c = (c >= numColors) ? numColors-1 : ((c < 0) ? 0 : c);

        // print the average temperature
        printf("%c",colors[c]);
      }
      // right wall
      printf("X\n");
    }

    // lower wall
    for(int i=0; i<W+2; i++) {
      printf("X");
    }
    printf("\n");

  }
  void neighbourMPI(int dimension, int* arraysize, int* subsize, int* substart, int* neighbour, MPI_Comm cartesian, double* buffer, enum mode mode){
    int neighbourrank;
    MPI_Datatype subarray;
    if (MPI_Cart_rank(cartesian, neighbour, &neighbourrank)){
      printf("can't query cartesian ranks");
      exit(EXIT_FAILURE);
    }
    if (MPI_Type_create_subarray(dimension, arraysize, subsize, substart, MPI_ORDER_C, MPI_DOUBLE, &subarray)){
      printf("can't create subarray");
      exit(EXIT_FAILURE);
    }
    if (MPI_Type_commit(&subarray)){
      printf("can't commit mpi type");
      exit(EXIT_FAILURE);
    }
    if (mode == SEND){
      if (MPI_Send(buffer, 1, subarray, neighbourrank, 0, MPI_COMM_WORLD)){
        printf("failure at mpi send");
        exit(EXIT_FAILURE);
      }
    } else {
      if (MPI_Recv(buffer, 1, subarray, neighbourrank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE)){
        printf("failure at mpi recv");
        exit(EXIT_FAILURE);
      }
    }
    if (MPI_Type_free(&subarray)){
      printf("failure free mpi type");
      exit(EXIT_FAILURE);
    }
  }

#+end_src



** Makefile
omp version is not used and shared is the version for exercise 2 which was not done :lazy:
#+begin_src makefile :eval never-export
  CC=mpicc
  CC_FLAGS=-O2 -g -std=gnu99 -Wall -Wextra -pedantic

  .PHONEY: all
  all: heat_stencil_2D_seq heat_stencil_2D_omp heat_stencil_2D_mpi heat_stencil_2D_mpi_hybrid heat_stencil_2D_mpi_shared

  heat_stencil_2D_seq: heat_stencil_2D_seq.c
    @$(CC) $(CC_FLAGS) $< -o heat_stencil_2D_seq

   heat_stencil_2D_mpi: heat_stencil_2D_mpi.c
    @$(CC) $(CC_FLAGS) $< -o heat_stencil_2D_mpi

  heat_stencil_2D_omp: heat_stencil_2D_omp.c
    @$(CC) $(CC_FLAGS) -fopenmp $< -o heat_stencil_2D_omp

  heat_stencil_2D_mpi_hybrid: heat_stencil_2D_mpi_hybrid.c 
    @$(CC) $(CC_FLAGS) -fopenmp $< -o heat_stencil_2D_mpi_hybrid

  heat_stencil_2D_mpi_shared: heat_stencil_2D_mpi_shared.c 
    @$(CC) $(CC_FLAGS) $< -o heat_stencil_2D_mpi_shared

  .PHONEY: clean
  clean:
    @rm heat_stencil_2D_seq
    @rm heat_stencil_2D_mpi
    @rm heat_stencil_2D_omp
    @rm heat_stencil_2D_mpi_hybrid
    @rm heat_stencil_2D_mpi_shared
#+end_src

** heat_stencil_2D.script
Binding was done so each rank runs on different node. So when running 8 ranks we utilize the whole system when threads are also 8.
#+begin_src bash :eval never-export
  #!/bin/bash

  # Execute job in the queue "std.q" unless you have special requirements.
  #$ -q std.q

  # The batch system should use the current directory as working directory.
  #$ -cwd

  # Name your job. Unless you use the -o and -e options, output will
  # go to a unique file name.ojob_id for each job.
  #$ -N kopp_heat_stencil_2D_hybrid

  ##$ -M markus.kopp@student.uibk.ac.at
  ##$ -m e

  # Join the error stream to the output stream.
  #$ -j yes

  #$ -pe openmpi-8perhost 64

  module load openmpi/4.0.3

  N=6000
  T=100
  echo "seq for comparison with N=$N x $N T=$T"
  time perf stat -d ./heat_stencil_2D_seq $N $N $T
  echo "--------------------"
  for FILE in heat_stencil_2D_mpi_hybrid
  do
      for XN in {1..8}; do
          for X in {2..8..2}; do
              echo "$FILE test with MPIrank=$XN and threads=$X and N=$Nx$N T=$T"
              time OMP_NUM_THREADS=$X mpiexec -n $XN --report-bindings --display-devel-map  --map-by numa:span --bind-to numa perf stat -d ./$FILE $N $N $T
              echo "--------------------"
          done
          echo "heat_stencil_2D_mpi test with MPIrank=$XN and threads=$X and N=$Nx$N T=$T"
          time OMP_NUM_THREADS=$X mpiexec -n $XN --report-bindings --display-devel-map --map-by numa:span --bind-to numa perf stat -d ./heat_stencil_2D_mpi $N $N $T
          echo "--------------------"
      done
  done

#+end_src

** Results
With 7 threads the problem size is not dividable. The name omp2, omp4, omp6, omp8 tells how many threads were used.
#+ATTR_LATEX: :width \textwidth :caption Execution time on lcc2 in seconds for N=6000x6000 T=100
| ranks |    seq |    mpi | mpi_hybrid_omp2 | mpi_hybrid_omp4 | mpi_hybrid_omp6 | mpi_hybrid_omp8 |
|-------+--------+--------+-----------------+-----------------+-----------------+-----------------|
|     1 | 49.963 | 51.284 |          26.842 |          15.033 |          13.349 |          13.006 |
|     2 | 49.963 | 26.539 |          14.379 |           8.385 |           7.631 |           7.527 |
|     3 | 49.963 | 18.429 |          10.159 |           6.193 |            5.77 |           5.707 |
|     4 | 49.963 | 14.498 |           8.313 |           5.184 |           5.319 |           4.928 |
|     5 | 49.963 | 11.902 |           6.928 |           4.463 |            4.22 |           4.191 |
|     6 | 49.963 | 10.307 |           6.246 |           4.285 |           4.074 |           4.369 |
|     7 |        |        |                 |                 |                 |                 |
|     8 | 49.963 |  8.285 |            5.18 |           3.939 |           3.579 |           3.823 |

#+ATTR_LATEX: :width \textwidth :caption Speedup with the same settings
| ranks | seq |   mpi | mpi_hybrid_omp2 | mpi_hybrid_omp4 | mpi_hybrid_omp6 | mpi_hybrid_omp8 |
|---------+-----+-------+-----------------+-----------------+-----------------+-----------------|
|       1 |   1 | 0.974 |           1.861 |           3.324 |           3.743 |           3.842 |
|       2 |   1 | 1.883 |           3.475 |           5.959 |           6.547 |           6.638 |
|       3 |   1 | 2.711 |           4.918 |           8.068 |           8.659 |           8.755 |
|       4 |   1 | 3.446 |           6.010 |           9.638 |           9.393 |          10.139 |
|       5 |   1 | 4.198 |           7.212 |          11.195 |          11.840 |          11.921 |
|       6 |   1 | 4.847 |           7.999 |          11.660 |          12.264 |          11.436 |
|       7 |     |       |                 |                 |                 |                 |
|       8 |   1 | 6.031 |           9.645 |          12.684 |          13.960 |          13.069 |

#+ATTR_LATEX: :width \textwidth :caption Efficiency with the same settings
| ranks | seq |   mpi | mpi_hybrid_omp2 | mpi_hybrid_omp4 | mpi_hybrid_omp6 | mpi_hybrid_omp8 |
|-------+-----+-------+-----------------+-----------------+-----------------+-----------------|
|     1 |   1 | 0.974 |           0.931 |           0.831 |           0.624 |           0.480 |
|     2 |   1 | 0.941 |           0.869 |           0.745 |           0.546 |           0.415 |
|     3 |   1 | 0.904 |           0.820 |           0.672 |           0.481 |           0.365 |
|     4 |   1 | 0.862 |           0.751 |           0.602 |           0.391 |           0.317 |
|     5 |   1 | 0.840 |           0.721 |           0.560 |           0.395 |           0.298 |
|     6 |   1 | 0.808 |           0.667 |           0.486 |           0.341 |           0.238 |
|     7 |     |       |                 |                 |                 |                 |
|     8 |   1 | 0.754 |           0.603 |           0.396 |           0.291 |           0.204 |


[[./graphs/mpivshyb.png]]
[[./graphs/mpivshybspeedup.png]]
[[./graphs/mpivshybefficiency.png]]
